# Fix LangSmith Tracing for OpenAI Responses API

⚠️ **Medium** - May need iteration, some complexity

## Problem Summary

LangSmith traces are not appearing despite environment variables being correctly configured. The codebase uses the OpenAI **Responses API** (`client.responses.create()`), which requires specific LangSmith SDK version support.

## Root Cause Analysis

1. **Responses API Support**: The `wrap_openai()` function only supports the Responses API starting from **langsmith 0.3.16**
2. **Version Constraint**: `requirements.txt` specifies `langsmith>=0.6.4` which should include support, but we need to verify the actual installed version
3. **Known Issues**: GitHub issue [#1588](https://github.com/langchain-ai/langsmith-sdk/issues/1588) indicates the feature was added but has some UI rendering issues
4. **Potential Configuration Issues**: EU endpoint, project name, or API key validation may need verification

## Fix Plan

### Task 1: Verify Installed Package Versions
**Action**: Check actual installed versions of `langsmith` and `openai` packages
**Command**:
```powershell
cd backend && pip show langsmith openai
```
**Validation**: Confirm `langsmith` version is ≥0.3.16 (ideally latest 0.6.x)

---

### Task 2: Update LangSmith to Latest Version
**Action**: Pin to a specific recent version known to support Responses API
**File**: `backend/requirements.txt`
**Change**: Update `langsmith>=0.6.4` to `langsmith==0.6.4` (or latest stable)
**Command**:
```powershell
cd backend && pip install --upgrade langsmith
```
**Validation**: Run `pip show langsmith` to confirm version

---

### Task 3: Add Debug Logging for LangSmith Initialization
**Action**: Add logging to verify LangSmith is properly initialized
**File**: `backend/app/services/langsmith.py`
**Changes**:
- Log the LangSmith SDK version at startup
- Log confirmation when wrap_openai is called
- Add a test trace on startup to verify connectivity

**Validation**: Check backend logs show LangSmith initialization messages

---

### Task 4: Verify Environment Variable Configuration
**Action**: Add startup validation to confirm all required env vars are set
**File**: `backend/app/services/langsmith.py`
**Changes**:
- Log masked API key (first 4 chars only) to confirm it's loaded
- Verify LANGSMITH_ENDPOINT is correct for your region (EU vs US)
- Check LANGSMITH_PROJECT matches your LangSmith dashboard

**Validation**: Backend logs show correct configuration on startup

---

### Task 5: Add Manual Tracing with @traceable Decorator (Fallback)
**Action**: If wrap_openai still doesn't work, use explicit `@traceable` decorator
**File**: `backend/app/services/openai_service.py`
**Changes**:
```python
from langsmith import traceable

@traceable(name="stream_chat_response", run_type="llm")
def stream_chat_response(...):
    ...

@traceable(name="get_chat_response", run_type="llm")
def get_chat_response(...):
    ...
```
**Validation**: Traces appear in LangSmith dashboard after making chat requests

---

### Task 6: Test End-to-End Tracing
**Action**: Make a test chat request and verify trace appears
**Steps**:
1. Start backend with `powershell -File scripts/start-backend.ps1`
2. Check backend logs for LangSmith initialization
3. Send a chat message via the frontend or curl
4. Check LangSmith dashboard for the trace

**Validation**:
- Trace visible in LangSmith project dashboard
- Shows request/response data
- Shows latency and token usage

---

### Task 7: (If Needed) Switch to Chat Completions API
**Action**: If Responses API tracing remains problematic, consider switching to Chat Completions API
**Rationale**: Chat Completions API (`client.chat.completions.create()`) has more mature LangSmith support
**Impact**: Would require refactoring streaming logic but is more battle-tested

**Validation**: Full tracing working with Chat Completions API

## Files to Modify

1. `backend/requirements.txt` - Update langsmith version
2. `backend/app/services/langsmith.py` - Add debug logging and validation
3. `backend/app/services/openai_service.py` - Add @traceable decorators if needed

## Success Criteria

- [ ] LangSmith traces appear in dashboard for chat requests
- [ ] Traces show input messages and output responses
- [ ] Traces show model, latency, and token usage
- [ ] No errors in backend logs related to LangSmith

## References

- [GitHub Issue #1588](https://github.com/langchain-ai/langsmith-sdk/issues/1588) - Responses API support
- [LangSmith wrap_openai docs](https://reference.langchain.com/python/langsmith/observability/sdk/wrappers/)
- [LangSmith Tracing Quickstart](https://docs.langchain.com/langsmith/observability-quickstart)
