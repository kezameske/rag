# Module 2: BYO Retrieval + Provider Abstraction

**Complexity:** ðŸ”´ Complex (5 phases, partially parallelizable)

## Overview

Replace OpenAI's managed RAG (Responses API + `file_search`) with:
1. Generic ChatCompletions API client (OpenRouter, Ollama, LM Studio, any OpenAI-compatible endpoint)
2. Custom ingestion pipeline (upload â†’ chunk â†’ embed â†’ pgvector)
3. Retrieval tool via standard ChatCompletions tool calling
4. Ingestion UI with Supabase Realtime status updates

## Execution Order

```
Stage A (parallel):   Phase 1 (Provider Abstraction)  ||  Phase 2 (Database Schema)
                               \                            /
Stage B (sequential):            Phase 3 (Ingestion Pipeline)
                               /                            \
Stage C (parallel):   Phase 4 (Retrieval Tool)  ||  Phase 5 (Ingestion UI)
```

| Stage | Phases | Dependencies |
|-------|--------|--------------|
| A | 1 + 2 in parallel | None - independent of each other |
| B | 3 sequentially | Needs Phase 1 (embedding client) + Phase 2 (tables) |
| C | 4 + 5 in parallel | Phase 4 needs 1+2+3; Phase 5 needs 2+3 (not 4) |

---

## Phase 1: Provider Abstraction (LLM Service Refactor) â€” Stage A

### Config changes (`backend/app/config.py`)

Add:
```python
# LLM Provider
llm_base_url: str = ""          # empty = OpenAI default
llm_api_key: str = ""           # empty = fallback to openai_api_key
llm_model: str = "gpt-4o"

# Embedding Provider (separate from LLM)
embedding_base_url: str = ""    # empty = OpenAI default
embedding_api_key: str = ""     # empty = fallback to openai_api_key
embedding_model: str = "text-embedding-3-small"
embedding_dimensions: int = 1536
```

Remove: `openai_vector_store_id` (no longer needed)

### Rename `openai_service.py` â†’ `llm_service.py`

Replace `client.responses.create()` with `client.chat.completions.create()`:

```python
async def astream_chat_response(
    messages: list[dict],
    tools: list[dict] | None = None,
) -> AsyncGenerator[dict, None]:
    settings = get_settings()

    request_kwargs = {
        "model": settings.llm_model,
        "messages": [{"role": "system", "content": SYSTEM_PROMPT}, *messages],
        "stream": True,
    }
    if tools:
        request_kwargs["tools"] = tools

    stream = await async_client.chat.completions.create(**request_kwargs)

    full_response = ""
    tool_calls_buffer = {}

    async for chunk in stream:
        delta = chunk.choices[0].delta if chunk.choices else None
        finish_reason = chunk.choices[0].finish_reason if chunk.choices else None

        if delta and delta.content:
            full_response += delta.content
            yield {"type": "text_delta", "content": delta.content}

        if delta and delta.tool_calls:
            for tc in delta.tool_calls:
                idx = tc.index
                if idx not in tool_calls_buffer:
                    tool_calls_buffer[idx] = {"id": tc.id, "name": tc.function.name, "arguments": ""}
                if tc.function and tc.function.arguments:
                    tool_calls_buffer[idx]["arguments"] += tc.function.arguments

        if finish_reason == "tool_calls":
            yield {"type": "tool_calls", "tool_calls": list(tool_calls_buffer.values())}

        if finish_reason == "stop":
            yield {"type": "response_completed", "content": full_response}
```

### Update `langsmith.py`

Modify client factories to accept `base_url` and `api_key`:
```python
def get_traced_openai_client(base_url: str | None = None, api_key: str | None = None) -> OpenAI:
    client = OpenAI(api_key=api_key, base_url=base_url or None)
    if settings.langsmith_api_key:
        return wrap_openai(client)
    return client
```

Create two clients in `llm_service.py`:
- **LLM client**: uses `llm_base_url` + `llm_api_key` (or fallback to `openai_api_key`)
- **Embedding client**: uses `embedding_base_url` + `embedding_api_key`

### Update `chat.py` router

- Change import from `openai_service` to `llm_service`
- Add tool-calling loop (execute tools, append results, re-call LLM until `finish_reason == "stop"`)

### Update `.env.example`

Add new vars, remove `OPENAI_VECTOR_STORE_ID`.

### Validation
- Set `LLM_BASE_URL=https://openrouter.ai/api/v1` with OpenRouter key
- Send a chat message, verify streaming works
- Check LangSmith traces appear
- Test without `LLM_BASE_URL` set (fallback to OpenAI ChatCompletions)

---

## Phase 2: Database Schema (pgvector + Documents + Chunks) â€” Stage A

### New migration: `supabase/migrations/20260123000000_module2_documents.sql`

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    filename TEXT NOT NULL,
    file_type TEXT NOT NULL,
    file_size INTEGER NOT NULL,
    storage_path TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending'
        CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
    error_message TEXT,
    chunk_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    embedding vector(1536),
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_documents_user_id ON documents(user_id);
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_chunks_document_id ON chunks(document_id);
CREATE INDEX idx_chunks_user_id ON chunks(user_id);
CREATE INDEX idx_chunks_embedding ON chunks
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- RLS
ALTER TABLE documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE chunks ENABLE ROW LEVEL SECURITY;

CREATE POLICY "users_own_documents" ON documents FOR ALL USING (auth.uid() = user_id);
CREATE POLICY "users_own_chunks" ON chunks FOR ALL USING (auth.uid() = user_id);

-- Realtime for ingestion status
ALTER PUBLICATION supabase_realtime ADD TABLE documents;

-- Vector search function
CREATE OR REPLACE FUNCTION match_chunks(
    query_embedding vector(1536),
    match_threshold float,
    match_count int,
    p_user_id uuid
) RETURNS TABLE (
    id uuid, document_id uuid, content text,
    chunk_index int, metadata jsonb, similarity float
) LANGUAGE plpgsql AS $$
BEGIN
    RETURN QUERY
    SELECT c.id, c.document_id, c.content, c.chunk_index, c.metadata,
           1 - (c.embedding <=> query_embedding) AS similarity
    FROM chunks c
    WHERE c.user_id = p_user_id
      AND 1 - (c.embedding <=> query_embedding) > match_threshold
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;
```

### Supabase Storage
Create a `documents` bucket. Policy: users upload/read/delete at path `{user_id}/*`.

### Validation
- Run `supabase db push`
- Verify tables + pgvector extension: `SELECT * FROM pg_extension WHERE extname = 'vector'`
- Insert a test document record, verify RLS

---

## Phase 3: Ingestion Pipeline â€” Stage B (after A completes)

### New files

| File | Purpose |
|------|---------|
| `backend/app/services/embedding_service.py` | Generate embeddings via configured provider |
| `backend/app/services/chunking_service.py` | Recursive character text splitter |
| `backend/app/services/ingestion_service.py` | Orchestrate: download â†’ extract â†’ chunk â†’ embed â†’ store |
| `backend/app/routers/documents.py` | Upload, list, delete endpoints |

### `embedding_service.py`
```python
async def get_embeddings(texts: list[str]) -> list[list[float]]:
    response = await embedding_client.embeddings.create(
        model=settings.embedding_model,
        input=texts,
        dimensions=settings.embedding_dimensions,
    )
    return [item.embedding for item in response.data]
```

### `chunking_service.py`
- Recursive split on `["\n\n", "\n", ". ", " "]`
- `chunk_size=1000`, `chunk_overlap=200`
- No external dependencies

### `ingestion_service.py`
1. Update document status â†’ `processing`
2. Download file from Supabase Storage
3. Extract text (Module 2: `.txt` and `.md` only)
4. Chunk the text
5. Batch embed chunks
6. Insert chunks with embeddings into `chunks` table
7. Update document status â†’ `completed` (or `failed` with error)

### `routers/documents.py`
- `POST /documents/upload` - Upload file, create record, trigger background processing via `BackgroundTasks`
- `GET /documents` - List user's documents
- `DELETE /documents/{document_id}` - Delete document + storage file (chunks cascade)

### Dependencies to add
```
python-multipart>=0.0.6
```

### Register router in `main.py`

### Validation
- Upload a `.txt` file via API
- Verify status transitions: pending â†’ processing â†’ completed
- Verify chunks created with non-null embeddings
- Delete document, verify cascade

---

## Phase 4: Retrieval Tool â€” Stage C

### New files

| File | Purpose |
|------|---------|
| `backend/app/services/retrieval_service.py` | Vector search via `match_chunks` RPC |
| `backend/app/services/tool_executor.py` | Execute tool calls, dispatch by name |

### `retrieval_service.py`
```python
async def search_documents(query: str, user_id: str, top_k: int = 5, threshold: float = 0.7) -> list[dict]:
    query_embedding = await get_embeddings([query])
    result = supabase.rpc("match_chunks", {
        "query_embedding": query_embedding[0],
        "match_threshold": threshold,
        "match_count": top_k,
        "p_user_id": user_id,
    }).execute()
    return result.data
```

### Tool definition (in `llm_service.py`)
```python
RAG_TOOLS = [{
    "type": "function",
    "function": {
        "name": "search_documents",
        "description": "Search the user's uploaded documents for relevant information.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "Search query"}
            },
            "required": ["query"]
        }
    }
}]
```

### `tool_executor.py`
Dispatch tool calls by name, format results for LLM context.

### Update chat router
Add tool-calling loop:
1. Stream LLM response
2. If `tool_calls` event: execute tools, append results to messages, call LLM again
3. Repeat until `finish_reason == "stop"` (max 3 rounds)
4. Only provide tools if user has documents uploaded

### Validation
- Upload a document about a specific topic
- Ask a question about that topic in chat
- Verify LLM calls `search_documents` tool
- Verify response references document content
- Check LangSmith shows tool-calling loop

---

## Phase 5: Ingestion UI + Realtime Status â€” Stage C

### New frontend files

| File | Purpose |
|------|---------|
| `frontend/src/pages/DocumentsPage.tsx` | Documents page layout |
| `frontend/src/components/documents/DocumentUpload.tsx` | Drag-and-drop upload |
| `frontend/src/components/documents/DocumentList.tsx` | File list with status |
| `frontend/src/hooks/useRealtimeDocuments.ts` | Supabase Realtime subscription |

### API additions (`frontend/src/lib/api.ts`)
- `uploadDocument(file: File)` - multipart POST
- `listDocuments()` - GET
- `deleteDocument(id)` - DELETE

### Types (`frontend/src/types/index.ts`)
- Add `Document` interface
- Remove unused `openai_thread_id`, `openai_message_id` fields from Thread/Message

### Realtime hook
Subscribe to `postgres_changes` on `documents` table filtered by `user_id`. Update local state on INSERT/UPDATE/DELETE events.

### UI Features
- Document list with status badges (pending/processing/completed/failed)
- Drag-and-drop upload zone
- File type validation (`.txt`, `.md` for Module 2)
- Delete with confirmation
- Chunk count display after processing

### Navigation
Add nav link in sidebar/header to switch between Chat and Documents views.
Update `App.tsx` routing: add `/documents` route.

### Validation
- Navigate to Documents page
- Upload a `.txt` file via drag-and-drop
- Watch status update in real-time (no refresh needed)
- Delete a document
- Go to Chat, ask about uploaded document content, get relevant answer

---

## Files Modified (Summary)

| File | Action |
|------|--------|
| `backend/app/config.py` | Add LLM + embedding config, remove vector_store_id |
| `backend/app/services/openai_service.py` | **Delete** (replaced by llm_service.py) |
| `backend/app/services/llm_service.py` | **New** - ChatCompletions streaming + tool support |
| `backend/app/services/langsmith.py` | Update client factories for base_url/api_key |
| `backend/app/services/embedding_service.py` | **New** - Embedding generation |
| `backend/app/services/chunking_service.py` | **New** - Text splitter |
| `backend/app/services/ingestion_service.py` | **New** - Ingestion orchestration |
| `backend/app/services/retrieval_service.py` | **New** - Vector search |
| `backend/app/services/tool_executor.py` | **New** - Tool dispatch |
| `backend/app/routers/chat.py` | Update imports, add tool-calling loop |
| `backend/app/routers/documents.py` | **New** - Document CRUD + upload |
| `backend/app/main.py` | Register documents router |
| `backend/app/models/schemas.py` | Add Document schemas |
| `backend/requirements.txt` | Add python-multipart |
| `backend/.env.example` | Add new env vars |
| `supabase/migrations/20260123..._module2.sql` | **New** - pgvector + documents + chunks |
| `frontend/src/types/index.ts` | Add Document type, remove OpenAI fields |
| `frontend/src/lib/api.ts` | Add document API functions |
| `frontend/src/pages/DocumentsPage.tsx` | **New** |
| `frontend/src/components/documents/DocumentUpload.tsx` | **New** |
| `frontend/src/components/documents/DocumentList.tsx` | **New** |
| `frontend/src/hooks/useRealtimeDocuments.ts` | **New** |
| `frontend/src/App.tsx` | Add /documents route + nav |

## End-to-End Verification

1. Start services: `powershell -File scripts/start-all.ps1`
2. Set env vars for OpenRouter (or Ollama) and OpenAI embeddings
3. Log in as test user
4. Navigate to Documents page
5. Upload a `.txt` file â†’ watch status go pending â†’ processing â†’ completed
6. Navigate to Chat â†’ create new thread
7. Ask a question about the uploaded document
8. Verify: LLM calls search_documents tool â†’ retrieves relevant chunks â†’ responds with sourced answer
9. Check LangSmith: traces show ChatCompletions calls with tool-calling loop
10. Test with Ollama: change `LLM_BASE_URL=http://localhost:11434/v1`, verify chat still works (without tools if model doesn't support them)
