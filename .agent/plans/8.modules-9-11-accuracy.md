# Plan 8: Modules 9–11 — Retrieval Accuracy Improvements

⚠️ **Medium** — Three focused enhancements, each modifying 1-2 files

## Context
Modules 1–8 complete. Retrieval pipeline works (hybrid search + reranking) but accuracy can be improved with three techniques: query transformation, contextual embeddings, and evaluation.

---

## Module 9: Query Transformation (HyDE)

### What
Before embedding the search query, generate a hypothetical document passage that would answer it. Embed that passage instead. This moves the query from "question space" into "document space" for better vector matching.

### New file: `backend/app/services/query_transform_service.py`
- `hyde_transform(query)` — calls LLM to generate 3-5 sentence hypothetical passage
- `expand_query(query)` — generates 2 reformulations for broader recall
- Both are non-fatal (fall back to raw query on failure)

### Modified: `backend/app/services/retrieval_service.py`
- Before embedding, call `hyde_transform()` and embed the hypothetical passage
- Keep original query for FTS (keyword matching benefits from original wording)
- Graceful fallback if HyDE fails

### Verify
- Chat query → backend logs show "Using HyDE-transformed query"
- Check LangSmith traces for the HyDE LLM call before embedding

---

## Module 10: Contextual Chunk Embeddings

### What
During ingestion, generate a short document-level context sentence, prepend it to each chunk before embedding. Store raw content for display, use contextual version for embedding only.

### Modified: `backend/app/services/ingestion_service.py`
- `_generate_document_context(text, filename)` — LLM generates 15-25 word context from first 2000 chars
- `_prepend_context(chunks, context)` — prepends `[Document context: ...]` to each chunk
- `process_document()` — generates context, embeds contextual chunks, stores raw content
- `doc_context` saved in chunk metadata for debugging

### Verify
- Upload new document → check chunk metadata has `doc_context` field
- Backend logs show "Document context for 'filename': ..."
- Retrieval should find more relevant chunks for ambiguous queries

### Note
Existing documents need re-ingestion to benefit. Delete and re-upload to get contextual embeddings.

---

## Module 11: Retrieval Evaluation Framework

### What
Automated eval: LLM generates questions from chunks, then measures if retrieval finds those chunks. Reports recall@5, recall@10, and MRR (Mean Reciprocal Rank).

### New file: `backend/app/services/eval_service.py`
- `EvalCase`, `EvalResult`, `EvalSummary` dataclasses
- `run_eval(user_id, eval_cases, top_k)` — runs full retrieval pipeline per case, measures keyword hits
- `auto_generate_eval_cases(user_id, max_cases)` — samples chunks, uses LLM to generate Q&A pairs

### New file: `backend/app/routers/eval.py`
- `POST /eval/run` — auto-generate cases + run eval (admin-only)
- `POST /eval/run-custom` — run with user-provided cases (admin-only)

### Modified: `backend/app/main.py`
- Register eval router

### Verify
- `curl -X POST /eval/run` → returns recall@5, recall@10, MRR, per-case results
- Custom eval: POST cases with known answers, verify correct hits

---

## Files Changed Summary

| File | M9 | M10 | M11 |
|------|-----|------|------|
| **New: query_transform_service.py** | ✓ | | |
| **New: eval_service.py** | | | ✓ |
| **New: routers/eval.py** | | | ✓ |
| retrieval_service.py | ✓ | | |
| ingestion_service.py | | ✓ | |
| main.py | | | ✓ |
| PRD.md | ✓ | ✓ | ✓ |
| PROGRESS.md | ✓ | ✓ | ✓ |
